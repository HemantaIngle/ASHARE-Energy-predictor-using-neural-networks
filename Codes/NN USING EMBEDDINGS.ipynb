{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOPIC- ASHRAE ENERGY PREDICTOR KAGGLE COMPETITION\n",
    "TEAM- GREENER EARTH -- HEMANTA INGLE\n",
    "MEMBERS- HEMANTA INGLE, SIDDHESH GAIKI\n",
    "GUIDED BY- PROF. BIRSEN SIRKECI\n",
    "BASE CODE- Credits given in readme file\n",
    "SOFTWARE USED- JUPYTER NOTEBOOK\n",
    "PURPOSE OF THE CODE- WHITH A PURPOSE OF COMBINING FEW MODELS TOGETHER ie ENSEMBLE , I HAVE TRIED ON FEW MODELS , TWEAKING SOME FETURES AND ADDING SOME PARAMETERS TO AN EXISTING WELL PERFORMING MODEL FROM KAGGLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ingle/JUPYTER/123/Best performing model_Neural Network.ipynb\n",
      "/Users/ingle/JUPYTER/123/building_metadata.csv\n",
      "/Users/ingle/JUPYTER/123/sample_submission.csv\n",
      "/Users/ingle/JUPYTER/123/test.csv\n",
      "/Users/ingle/JUPYTER/123/train.csv\n",
      "/Users/ingle/JUPYTER/123/weather_test.csv\n",
      "/Users/ingle/JUPYTER/123/weather_train.csv\n",
      "/Users/ingle/JUPYTER/123/.ipynb_checkpoints\\Best performing model_Neural Network-checkpoint.ipynb\n"
     ]
    }
   ],
   "source": [
    "# IMPORTING THE NECESSARY LIBRARIES\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/Users/ingle/JUPYTER/123/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dropout, Dense, Embedding, SpatialDropout1D, concatenate, BatchNormalization, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.losses import mean_squared_error as mse_loss\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_imputation(df, column_name):\n",
    "    imputation = df.groupby(['timestamp'])[column_name].mean()\n",
    "    \n",
    "    df.loc[df[column_name].isnull(), column_name] = df[df[column_name].isnull()][[column_name]].apply(lambda x: imputation[df['timestamp'][x.index]].values)\n",
    "    del imputation\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining building and weather files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "building_df = pd.read_csv(\"/Users/ingle/JUPYTER/123/building_metadata.csv\")\n",
    "weather_train = pd.read_csv(\"/Users/ingle/JUPYTER/123/weather_train.csv\")\n",
    "train = pd.read_csv(\"/Users/ingle/JUPYTER/123/train.csv\")\n",
    "\n",
    "train = train.merge(building_df, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\n",
    "train = train.merge(weather_train, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"])\n",
    "del weather_train\n",
    "\n",
    "train.loc[(train['meter']==0) & (train['site_id']==0) & (train['timestamp']<'2016-05-21 00:00:00'), 'drop'] = True\n",
    "train = train[train['drop']!=True]\n",
    "\n",
    "train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\n",
    "train[\"hour\"] = train[\"timestamp\"].dt.hour\n",
    "train[\"weekday\"] = train[\"timestamp\"].dt.weekday\n",
    "\n",
    "train = average_imputation(train, 'wind_speed')\n",
    "\n",
    "beaufort = [(0, 0, 0.3), (1, 0.3, 1.6), (2, 1.6, 3.4), (3, 3.4, 5.5), (4, 5.5, 8), (5, 8, 10.8), (6, 10.8, 13.9), \n",
    "          (7, 13.9, 17.2), (8, 17.2, 20.8), (9, 20.8, 24.5), (10, 24.5, 28.5), (11, 28.5, 33), (12, 33, 200)]\n",
    "\n",
    "for item in beaufort:\n",
    "    train.loc[(train['wind_speed']>=item[1]) & (train['wind_speed']<item[2]), 'beaufort_scale'] = item[0]\n",
    "\n",
    "del train[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL AND NUMERICAL VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder# Handling categorical and numerical values\n",
    "\n",
    "le = LabelEncoder()\n",
    "train[\"primary_use\"] = le.fit_transform(train[\"primary_use\"])\n",
    "\n",
    "categoricals = [\"site_id\", \"building_id\", \"primary_use\", \"hour\", \"weekday\",  \"meter\"]\n",
    "\n",
    "drop_cols = [\"sea_level_pressure\", \"wind_speed\", \"wind_direction\"]\n",
    "\n",
    "numericals = [\"square_feet\", \"year_built\", \"air_temperature\", \"cloud_coverage\",\n",
    "              \"dew_temperature\", \"precip_depth_1_hr\", \"floor_count\", 'beaufort_scale']\n",
    "\n",
    "feat_cols = categoricals + numericals # Combining the both values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.log1p(train[\"meter_reading\"])\n",
    "\n",
    "del train[\"meter_reading\"] \n",
    "\n",
    "train = train.drop(drop_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIMIZING MEMORY\n",
    "REFERNCE TAKEN FROM https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on this great kernel \n",
    "def reduce_mem_usage(df):\n",
    "    start_mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != object:  # Exclude strings            \n",
    "            # Print current column type\n",
    "            print(\"******************************\")\n",
    "            print(\"Column: \",col)\n",
    "            print(\"dtype before: \",df[col].dtype)            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = df[col].max()\n",
    "            mn = df[col].min()\n",
    "            print(\"min for this col: \",mn)\n",
    "            print(\"max for this col: \",mx)\n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(df[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                df[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = df[col].fillna(0).astype(np.int64)\n",
    "            result = (df[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        df[col] = df[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        df[col] = df[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        df[col] = df[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)    \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            \n",
    "            # Print new column type\n",
    "            print(\"dtype after: \",df[col].dtype)\n",
    "            print(\"******************************\")\n",
    "    # Print final result\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return df, NAlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 2978.74662399292  MB\n",
      "******************************\n",
      "Column:  building_id\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  1448\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  meter\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  3\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  site_id\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  15\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  primary_use\n",
      "dtype before:  int32\n",
      "min for this col:  0\n",
      "max for this col:  15\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  square_feet\n",
      "dtype before:  int64\n",
      "min for this col:  283\n",
      "max for this col:  875000\n",
      "dtype after:  uint32\n",
      "******************************\n",
      "******************************\n",
      "Column:  year_built\n",
      "dtype before:  float64\n",
      "min for this col:  1900.0\n",
      "max for this col:  2017.0\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  floor_count\n",
      "dtype before:  float64\n",
      "min for this col:  1.0\n",
      "max for this col:  26.0\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  air_temperature\n",
      "dtype before:  float64\n",
      "min for this col:  -28.9\n",
      "max for this col:  47.2\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  cloud_coverage\n",
      "dtype before:  float64\n",
      "min for this col:  0.0\n",
      "max for this col:  9.0\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  dew_temperature\n",
      "dtype before:  float64\n",
      "min for this col:  -35.0\n",
      "max for this col:  26.1\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  precip_depth_1_hr\n",
      "dtype before:  float64\n",
      "min for this col:  -1.0\n",
      "max for this col:  343.0\n",
      "dtype after:  int16\n",
      "******************************\n",
      "******************************\n",
      "Column:  hour\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  23\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  weekday\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  6\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  beaufort_scale\n",
      "dtype before:  float64\n",
      "min for this col:  0.0\n",
      "max for this col:  8.0\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  1432.156114578247  MB\n",
      "This is  48.07915191720755 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "train, NAlist = reduce_mem_usage(train)# REDUCING THE MEMORY USAGE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING A NEURAL NETWORK MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(dense_dim_1=128, dense_dim_2=64, dense_dim_3=64, dense_dim_4=32, \n",
    "dropout1=0.2, dropout2=0.1, dropout3=0.1, dropout4=0.1, lr=0.001):\n",
    "\n",
    "    #Inputs\n",
    "    site_id = Input(shape=[1], name=\"site_id\")\n",
    "    building_id = Input(shape=[1], name=\"building_id\")\n",
    "    meter = Input(shape=[1], name=\"meter\")\n",
    "    primary_use = Input(shape=[1], name=\"primary_use\")\n",
    "    square_feet = Input(shape=[1], name=\"square_feet\")\n",
    "    year_built = Input(shape=[1], name=\"year_built\")\n",
    "    air_temperature = Input(shape=[1], name=\"air_temperature\")\n",
    "    cloud_coverage = Input(shape=[1], name=\"cloud_coverage\")\n",
    "    dew_temperature = Input(shape=[1], name=\"dew_temperature\")\n",
    "    hour = Input(shape=[1], name=\"hour\")\n",
    "    precip = Input(shape=[1], name=\"precip_depth_1_hr\")\n",
    "    weekday = Input(shape=[1], name=\"weekday\")\n",
    "    beaufort_scale = Input(shape=[1], name=\"beaufort_scale\")\n",
    "   \n",
    "    #Embeddings layers\n",
    "    emb_site_id = Embedding(16, 2)(site_id)\n",
    "    emb_building_id = Embedding(1449, 6)(building_id)\n",
    "    emb_meter = Embedding(4, 2)(meter)\n",
    "    emb_primary_use = Embedding(16, 2)(primary_use)\n",
    "    emb_hour = Embedding(24, 3)(hour)\n",
    "    emb_weekday = Embedding(7, 2)(weekday)\n",
    "\n",
    "    concat_emb = concatenate([\n",
    "           Flatten() (emb_site_id)\n",
    "         , Flatten() (emb_building_id)\n",
    "         , Flatten() (emb_meter)\n",
    "         , Flatten() (emb_primary_use)\n",
    "         , Flatten() (emb_hour)\n",
    "         , Flatten() (emb_weekday)\n",
    "    ])\n",
    "    \n",
    "    categ = Dropout(dropout1)(Dense(dense_dim_1,activation='relu') (concat_emb))\n",
    "    categ = BatchNormalization()(categ)\n",
    "    categ = Dropout(dropout2)(Dense(dense_dim_2,activation='relu') (categ))\n",
    "    \n",
    "    #main layer\n",
    "    main_l = concatenate([\n",
    "          categ\n",
    "        , square_feet\n",
    "        , year_built\n",
    "        , air_temperature\n",
    "        , cloud_coverage\n",
    "        , dew_temperature\n",
    "        , precip\n",
    "        , beaufort_scale\n",
    "    ])\n",
    "    \n",
    "    main_l = Dropout(dropout3)(Dense(dense_dim_3,activation='relu') (main_l))# DEFINING THE LAYERS \n",
    "    main_l = BatchNormalization()(main_l)\n",
    "    main_l = Dropout(dropout4)(Dense(dense_dim_4,activation='relu') (main_l))\n",
    "    \n",
    "    #output\n",
    "    output = Dense(1) (main_l)\n",
    "\n",
    "    model = Model([ site_id,\n",
    "                    building_id, \n",
    "                    meter, \n",
    "                    primary_use, \n",
    "                    square_feet, \n",
    "                    year_built, \n",
    "                    air_temperature,\n",
    "                    cloud_coverage,\n",
    "                    dew_temperature, \n",
    "                    hour,\n",
    "                    weekday, \n",
    "                    precip,\n",
    "                    beaufort_scale], output)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr=lr),\n",
    "                  loss= mse_loss,\n",
    "                  metrics=[root_mean_squared_error])\n",
    "    return model\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_data(df, num_cols, cat_cols):\n",
    "    cols = num_cols + cat_cols\n",
    "    X = {col: np.array(df[col]) for col in cols}\n",
    "    return X\n",
    "\n",
    "def train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold, patience=3):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=1)\n",
    "    model_checkpoint = ModelCheckpoint(\"model_\" + str(fold) + \".hdf5\",\n",
    "                                       save_best_only=True, verbose=1, monitor='val_root_mean_squared_error', mode='min')\n",
    "\n",
    "    hist = keras_model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                            validation_data=(X_v, y_valid), verbose=1,\n",
    "                            callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "    keras_model = load_model(\"model_\" + str(fold) + \".hdf5\", custom_objects={'root_mean_squared_error': root_mean_squared_error})\n",
    "    \n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING FOLDS OF DATA FOR CROSSVALIIDATION AND TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "WARNING:tensorflow:From C:\\Users\\ingle\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ingle\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ingle\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ingle\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ingle\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\ingle\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ingle\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 14832268 samples, validate on 4944777 samples\n",
      "Epoch 1/25\n",
      "14832268/14832268 [==============================] - 142s 10us/step - loss: 1.8721 - root_mean_squared_error: 1.3394 - val_loss: 1.4382 - val_root_mean_squared_error: 1.1834\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 1.18336, saving model to model_0.hdf5\n",
      "Epoch 2/25\n",
      "14832268/14832268 [==============================] - 137s 9us/step - loss: 1.5341 - root_mean_squared_error: 1.2373 - val_loss: 1.4287 - val_root_mean_squared_error: 1.1800\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 1.18336 to 1.17995, saving model to model_0.hdf5\n",
      "Epoch 3/25\n",
      "14832268/14832268 [==============================] - 140s 9us/step - loss: 1.4618 - root_mean_squared_error: 1.2075 - val_loss: 1.2021 - val_root_mean_squared_error: 1.0797\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error improved from 1.17995 to 1.07965, saving model to model_0.hdf5\n",
      "Epoch 4/25\n",
      "14832268/14832268 [==============================] - 139s 9us/step - loss: 1.2706 - root_mean_squared_error: 1.1256 - val_loss: 1.0917 - val_root_mean_squared_error: 1.0254\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error improved from 1.07965 to 1.02545, saving model to model_0.hdf5\n",
      "Epoch 5/25\n",
      "14832268/14832268 [==============================] - 131s 9us/step - loss: 1.2029 - root_mean_squared_error: 1.0952 - val_loss: 1.0270 - val_root_mean_squared_error: 0.9933\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error improved from 1.02545 to 0.99334, saving model to model_0.hdf5\n",
      "Epoch 6/25\n",
      "14832268/14832268 [==============================] - 132s 9us/step - loss: 1.1743 - root_mean_squared_error: 1.0821 - val_loss: 1.0258 - val_root_mean_squared_error: 0.9943\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 0.99334\n",
      "Epoch 7/25\n",
      "14832268/14832268 [==============================] - 128s 9us/step - loss: 1.1544 - root_mean_squared_error: 1.0728 - val_loss: 0.9977 - val_root_mean_squared_error: 0.9790\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error improved from 0.99334 to 0.97899, saving model to model_0.hdf5\n",
      "Epoch 8/25\n",
      "14832268/14832268 [==============================] - 130s 9us/step - loss: 1.1431 - root_mean_squared_error: 1.0676 - val_loss: 1.0000 - val_root_mean_squared_error: 0.9801\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.97899\n",
      "Epoch 9/25\n",
      "14832268/14832268 [==============================] - 129s 9us/step - loss: 1.1314 - root_mean_squared_error: 1.0620 - val_loss: 0.9555 - val_root_mean_squared_error: 0.9544\n",
      "\n",
      "Epoch 00009: val_root_mean_squared_error improved from 0.97899 to 0.95441, saving model to model_0.hdf5\n",
      "Epoch 10/25\n",
      "14832268/14832268 [==============================] - 127s 9us/step - loss: 1.1214 - root_mean_squared_error: 1.0574 - val_loss: 1.0097 - val_root_mean_squared_error: 0.9832\n",
      "\n",
      "Epoch 00010: val_root_mean_squared_error did not improve from 0.95441\n",
      "Epoch 11/25\n",
      "14832268/14832268 [==============================] - 129s 9us/step - loss: 1.1136 - root_mean_squared_error: 1.0536 - val_loss: 0.9698 - val_root_mean_squared_error: 0.9644\n",
      "\n",
      "Epoch 00011: val_root_mean_squared_error did not improve from 0.95441\n",
      "Epoch 12/25\n",
      "14832268/14832268 [==============================] - 127s 9us/step - loss: 1.1048 - root_mean_squared_error: 1.0495 - val_loss: 0.9896 - val_root_mean_squared_error: 0.9726\n",
      "\n",
      "Epoch 00012: val_root_mean_squared_error did not improve from 0.95441\n",
      "Epoch 00012: early stopping\n",
      "**************************************************\n",
      "Fold: 1\n",
      "Train on 14832479 samples, validate on 4944566 samples\n",
      "Epoch 1/25\n",
      "14832479/14832479 [==============================] - 138s 9us/step - loss: 1.8883 - root_mean_squared_error: 1.3478 - val_loss: 1.4434 - val_root_mean_squared_error: 1.1860\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 1.18602, saving model to model_1.hdf5\n",
      "Epoch 2/25\n",
      "14832479/14832479 [==============================] - 135s 9us/step - loss: 1.5389 - root_mean_squared_error: 1.2392 - val_loss: 1.4401 - val_root_mean_squared_error: 1.1837\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 1.18602 to 1.18370, saving model to model_1.hdf5\n",
      "Epoch 3/25\n",
      "14832479/14832479 [==============================] - 137s 9us/step - loss: 1.4191 - root_mean_squared_error: 1.1892 - val_loss: 1.0946 - val_root_mean_squared_error: 1.0274\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error improved from 1.18370 to 1.02743, saving model to model_1.hdf5\n",
      "Epoch 4/25\n",
      "14832479/14832479 [==============================] - 135s 9us/step - loss: 1.2098 - root_mean_squared_error: 1.0983 - val_loss: 1.0269 - val_root_mean_squared_error: 0.9930\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error improved from 1.02743 to 0.99299, saving model to model_1.hdf5\n",
      "Epoch 5/25\n",
      "14832479/14832479 [==============================] - 134s 9us/step - loss: 1.1579 - root_mean_squared_error: 1.0745 - val_loss: 1.0122 - val_root_mean_squared_error: 0.9847\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error improved from 0.99299 to 0.98474, saving model to model_1.hdf5\n",
      "Epoch 6/25\n",
      "14832479/14832479 [==============================] - 137s 9us/step - loss: 1.1330 - root_mean_squared_error: 1.0628 - val_loss: 1.0119 - val_root_mean_squared_error: 0.9843\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error improved from 0.98474 to 0.98434, saving model to model_1.hdf5\n",
      "Epoch 7/25\n",
      "14832479/14832479 [==============================] - 137s 9us/step - loss: 1.1131 - root_mean_squared_error: 1.0534 - val_loss: 0.9928 - val_root_mean_squared_error: 0.9753\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error improved from 0.98434 to 0.97530, saving model to model_1.hdf5\n",
      "Epoch 8/25\n",
      "14832479/14832479 [==============================] - 135s 9us/step - loss: 1.1031 - root_mean_squared_error: 1.0487 - val_loss: 0.9732 - val_root_mean_squared_error: 0.9647\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error improved from 0.97530 to 0.96465, saving model to model_1.hdf5\n",
      "Epoch 9/25\n",
      "14832479/14832479 [==============================] - 135s 9us/step - loss: 1.0962 - root_mean_squared_error: 1.0454 - val_loss: 1.0015 - val_root_mean_squared_error: 0.9816\n",
      "\n",
      "Epoch 00009: val_root_mean_squared_error did not improve from 0.96465\n",
      "Epoch 10/25\n",
      "14832479/14832479 [==============================] - 145s 10us/step - loss: 1.0915 - root_mean_squared_error: 1.0431 - val_loss: 0.9692 - val_root_mean_squared_error: 0.9633\n",
      "\n",
      "Epoch 00010: val_root_mean_squared_error improved from 0.96465 to 0.96326, saving model to model_1.hdf5\n",
      "Epoch 11/25\n",
      "14832479/14832479 [==============================] - 137s 9us/step - loss: 1.0873 - root_mean_squared_error: 1.0411 - val_loss: 0.9908 - val_root_mean_squared_error: 0.9750\n",
      "\n",
      "Epoch 00011: val_root_mean_squared_error did not improve from 0.96326\n",
      "Epoch 12/25\n",
      "14832479/14832479 [==============================] - 136s 9us/step - loss: 1.0840 - root_mean_squared_error: 1.0395 - val_loss: 0.9427 - val_root_mean_squared_error: 0.9480\n",
      "\n",
      "Epoch 00012: val_root_mean_squared_error improved from 0.96326 to 0.94798, saving model to model_1.hdf5\n",
      "Epoch 13/25\n",
      "14832479/14832479 [==============================] - 141s 9us/step - loss: 1.0803 - root_mean_squared_error: 1.0377 - val_loss: 0.9488 - val_root_mean_squared_error: 0.9516\n",
      "\n",
      "Epoch 00013: val_root_mean_squared_error did not improve from 0.94798\n",
      "Epoch 14/25\n",
      "14832479/14832479 [==============================] - 136s 9us/step - loss: 1.0782 - root_mean_squared_error: 1.0367 - val_loss: 0.9556 - val_root_mean_squared_error: 0.9562\n",
      "\n",
      "Epoch 00014: val_root_mean_squared_error did not improve from 0.94798\n",
      "Epoch 15/25\n",
      "14832479/14832479 [==============================] - 136s 9us/step - loss: 1.0760 - root_mean_squared_error: 1.0356 - val_loss: 0.9699 - val_root_mean_squared_error: 0.9644\n",
      "\n",
      "Epoch 00015: val_root_mean_squared_error did not improve from 0.94798\n",
      "Epoch 00015: early stopping\n",
      "**************************************************\n",
      "Fold: 2\n",
      "Train on 14832969 samples, validate on 4944076 samples\n",
      "Epoch 1/25\n",
      "14832969/14832969 [==============================] - 144s 10us/step - loss: 1.9105 - root_mean_squared_error: 1.3520 - val_loss: 1.4419 - val_root_mean_squared_error: 1.1853\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 1.18527, saving model to model_2.hdf5\n",
      "Epoch 2/25\n",
      "14832969/14832969 [==============================] - 150s 10us/step - loss: 1.5204 - root_mean_squared_error: 1.2316 - val_loss: 1.3129 - val_root_mean_squared_error: 1.1330\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 1.18527 to 1.13305, saving model to model_2.hdf5\n",
      "Epoch 3/25\n",
      "14832969/14832969 [==============================] - 165s 11us/step - loss: 1.2950 - root_mean_squared_error: 1.1363 - val_loss: 1.0566 - val_root_mean_squared_error: 1.0083\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error improved from 1.13305 to 1.00825, saving model to model_2.hdf5\n",
      "Epoch 4/25\n",
      "14832969/14832969 [==============================] - 189s 13us/step - loss: 1.2077 - root_mean_squared_error: 1.0974 - val_loss: 1.0316 - val_root_mean_squared_error: 0.9954\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error improved from 1.00825 to 0.99537, saving model to model_2.hdf5\n",
      "Epoch 5/25\n",
      "14832969/14832969 [==============================] - 167s 11us/step - loss: 1.1855 - root_mean_squared_error: 1.0872 - val_loss: 1.0011 - val_root_mean_squared_error: 0.9791\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error improved from 0.99537 to 0.97909, saving model to model_2.hdf5\n",
      "Epoch 6/25\n",
      "14832969/14832969 [==============================] - 144s 10us/step - loss: 1.1700 - root_mean_squared_error: 1.0801 - val_loss: 1.0354 - val_root_mean_squared_error: 0.9973\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error did not improve from 0.97909\n",
      "Epoch 7/25\n",
      "14832969/14832969 [==============================] - 146s 10us/step - loss: 1.1606 - root_mean_squared_error: 1.0757 - val_loss: 1.0257 - val_root_mean_squared_error: 0.9947\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.97909\n",
      "Epoch 8/25\n",
      "14832969/14832969 [==============================] - 143s 10us/step - loss: 1.1503 - root_mean_squared_error: 1.0709 - val_loss: 0.9994 - val_root_mean_squared_error: 0.9797\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.97909\n",
      "Epoch 9/25\n",
      "14832969/14832969 [==============================] - 144s 10us/step - loss: 1.1389 - root_mean_squared_error: 1.0655 - val_loss: 1.0131 - val_root_mean_squared_error: 0.9874\n",
      "\n",
      "Epoch 00009: val_root_mean_squared_error did not improve from 0.97909\n",
      "Epoch 10/25\n",
      "14832969/14832969 [==============================] - 143s 10us/step - loss: 1.1266 - root_mean_squared_error: 1.0597 - val_loss: 0.9915 - val_root_mean_squared_error: 0.9739\n",
      "\n",
      "Epoch 00010: val_root_mean_squared_error improved from 0.97909 to 0.97386, saving model to model_2.hdf5\n",
      "Epoch 11/25\n",
      "14832969/14832969 [==============================] - 143s 10us/step - loss: 1.1186 - root_mean_squared_error: 1.0560 - val_loss: 1.0106 - val_root_mean_squared_error: 0.9860\n",
      "\n",
      "Epoch 00011: val_root_mean_squared_error did not improve from 0.97386\n",
      "Epoch 12/25\n",
      "14832969/14832969 [==============================] - 144s 10us/step - loss: 1.1119 - root_mean_squared_error: 1.0528 - val_loss: 0.9737 - val_root_mean_squared_error: 0.9629\n",
      "\n",
      "Epoch 00012: val_root_mean_squared_error improved from 0.97386 to 0.96295, saving model to model_2.hdf5\n",
      "Epoch 13/25\n",
      "14832969/14832969 [==============================] - 157s 11us/step - loss: 1.1073 - root_mean_squared_error: 1.0506 - val_loss: 0.9996 - val_root_mean_squared_error: 0.9811\n",
      "\n",
      "Epoch 00013: val_root_mean_squared_error did not improve from 0.96295\n",
      "Epoch 14/25\n",
      "14832969/14832969 [==============================] - 157s 11us/step - loss: 1.1031 - root_mean_squared_error: 1.0486 - val_loss: 0.9892 - val_root_mean_squared_error: 0.9754_mean_sq - ETA: 4s - loss: 1.1032 - root_mean\n",
      "\n",
      "Epoch 00014: val_root_mean_squared_error did not improve from 0.96295\n",
      "Epoch 15/25\n",
      "14832969/14832969 [==============================] - 200s 13us/step - loss: 1.0995 - root_mean_squared_error: 1.0469 - val_loss: 0.9789 - val_root_mean_squared_error: 0.9695\n",
      "\n",
      "Epoch 00015: val_root_mean_squared_error did not improve from 0.96295\n",
      "Epoch 00015: early stopping\n",
      "**************************************************\n",
      "Fold: 3\n",
      "Train on 14833419 samples, validate on 4943626 samples\n",
      "Epoch 1/25\n",
      "14833419/14833419 [==============================] - 177s 12us/step - loss: 1.8386 - root_mean_squared_error: 1.3371 - val_loss: 1.4482 - val_root_mean_squared_error: 1.1871\n",
      "\n",
      "Epoch 00001: val_root_mean_squared_error improved from inf to 1.18705, saving model to model_3.hdf5\n",
      "Epoch 2/25\n",
      "14833419/14833419 [==============================] - 176s 12us/step - loss: 1.5352 - root_mean_squared_error: 1.2377 - val_loss: 1.2793 - val_root_mean_squared_error: 1.1146\n",
      "\n",
      "Epoch 00002: val_root_mean_squared_error improved from 1.18705 to 1.11465, saving model to model_3.hdf5\n",
      "Epoch 3/25\n",
      "14833419/14833419 [==============================] - 181s 12us/step - loss: 1.3107 - root_mean_squared_error: 1.1433 - val_loss: 1.0899 - val_root_mean_squared_error: 1.0267\n",
      "\n",
      "Epoch 00003: val_root_mean_squared_error improved from 1.11465 to 1.02671, saving model to model_3.hdf5\n",
      "Epoch 4/25\n",
      "14833419/14833419 [==============================] - 158s 11us/step - loss: 1.2041 - root_mean_squared_error: 1.0957 - val_loss: 1.0822 - val_root_mean_squared_error: 1.0208\n",
      "\n",
      "Epoch 00004: val_root_mean_squared_error improved from 1.02671 to 1.02079, saving model to model_3.hdf5\n",
      "Epoch 5/25\n",
      "14833419/14833419 [==============================] - 154s 10us/step - loss: 1.1703 - root_mean_squared_error: 1.0802 - val_loss: 1.0729 - val_root_mean_squared_error: 1.0173\n",
      "\n",
      "Epoch 00005: val_root_mean_squared_error improved from 1.02079 to 1.01728, saving model to model_3.hdf5\n",
      "Epoch 6/25\n",
      "14833419/14833419 [==============================] - 156s 11us/step - loss: 1.1514 - root_mean_squared_error: 1.0714 - val_loss: 0.9973 - val_root_mean_squared_error: 0.9777\n",
      "\n",
      "Epoch 00006: val_root_mean_squared_error improved from 1.01728 to 0.97769, saving model to model_3.hdf5\n",
      "Epoch 7/25\n",
      "14833419/14833419 [==============================] - 151s 10us/step - loss: 1.1288 - root_mean_squared_error: 1.0609 - val_loss: 1.0405 - val_root_mean_squared_error: 0.9997\n",
      "\n",
      "Epoch 00007: val_root_mean_squared_error did not improve from 0.97769\n",
      "Epoch 8/25\n",
      "14833419/14833419 [==============================] - 151s 10us/step - loss: 1.1135 - root_mean_squared_error: 1.0536 - val_loss: 1.0221 - val_root_mean_squared_error: 0.9892\n",
      "\n",
      "Epoch 00008: val_root_mean_squared_error did not improve from 0.97769\n",
      "Epoch 9/25\n",
      "14833419/14833419 [==============================] - 151s 10us/step - loss: 1.1048 - root_mean_squared_error: 1.0495 - val_loss: 1.0162 - val_root_mean_squared_error: 0.9861\n",
      "\n",
      "Epoch 00009: val_root_mean_squared_error did not improve from 0.97769\n",
      "Epoch 00009: early stopping\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "oof = np.zeros(len(train))\n",
    "batch_size = 1024\n",
    "epochs = 25\n",
    "models = []\n",
    "\n",
    "folds = 4\n",
    "seed = 666\n",
    "\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(kf.split(train, train['building_id'])):\n",
    "    print('Fold:', fold_n)\n",
    "    X_train, X_valid = train.iloc[train_index], train.iloc[valid_index]\n",
    "    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n",
    "    X_t = get_keras_data(X_train, numericals, categoricals)\n",
    "    X_v = get_keras_data(X_valid, numericals, categoricals)\n",
    "    \n",
    "    keras_model = model(dense_dim_1=64, dense_dim_2=32, dense_dim_3=32, dense_dim_4=16, \n",
    "                        dropout1=0.2, dropout2=0.1, dropout3=0.1, dropout4=0.1, lr=0.001)\n",
    "    mod = train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold_n, patience=3)\n",
    "    models.append(mod)\n",
    "    print('*'* 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "#del train, target, X_train, X_valid, y_train, y_valid, X_t, X_v, kf\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_df = pd.read_csv(\"/Users/ingle/JUPYTER/123/building_metadata.csv\")\n",
    "test = pd.read_csv(\"/Users/ingle/JUPYTER/123/test.csv\")\n",
    "\n",
    "test = test.merge(building_df, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\n",
    "del building_df\n",
    "gc.collect()\n",
    "test[\"primary_use\"] = le.transform(test[\"primary_use\"])\n",
    "\n",
    "weather_test = pd.read_csv(\"/Users/ingle/JUPYTER/123/weather_test.csv\")\n",
    "\n",
    "test = test.merge(weather_test, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"], how = \"left\")\n",
    "del weather_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 5892.847900390625  MB\n",
      "******************************\n",
      "Column:  site_id\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  15\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  building_id\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  1448\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  primary_use\n",
      "dtype before:  int32\n",
      "min for this col:  0\n",
      "max for this col:  15\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  hour\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  23\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  weekday\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  6\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  meter\n",
      "dtype before:  int64\n",
      "min for this col:  0\n",
      "max for this col:  3\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  square_feet\n",
      "dtype before:  int64\n",
      "min for this col:  283\n",
      "max for this col:  875000\n",
      "dtype after:  uint32\n",
      "******************************\n",
      "******************************\n",
      "Column:  year_built\n",
      "dtype before:  float64\n",
      "min for this col:  1900.0\n",
      "max for this col:  2017.0\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  air_temperature\n",
      "dtype before:  float64\n",
      "min for this col:  -28.1\n",
      "max for this col:  48.3\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  cloud_coverage\n",
      "dtype before:  float64\n",
      "min for this col:  0.0\n",
      "max for this col:  9.0\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  dew_temperature\n",
      "dtype before:  float64\n",
      "min for this col:  -31.6\n",
      "max for this col:  26.7\n",
      "dtype after:  float32\n",
      "******************************\n",
      "******************************\n",
      "Column:  precip_depth_1_hr\n",
      "dtype before:  float64\n",
      "min for this col:  -1.0\n",
      "max for this col:  597.0\n",
      "dtype after:  int16\n",
      "******************************\n",
      "******************************\n",
      "Column:  floor_count\n",
      "dtype before:  float64\n",
      "min for this col:  1.0\n",
      "max for this col:  26.0\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  beaufort_scale\n",
      "dtype before:  float64\n",
      "min for this col:  0.0\n",
      "max for this col:  9.0\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  2632.0416259765625  MB\n",
      "This is  44.66501885789535 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "test[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\n",
    "test[\"hour\"] = test[\"timestamp\"].dt.hour\n",
    "test[\"weekday\"] = test[\"timestamp\"].dt.weekday\n",
    "\n",
    "test = average_imputation(test, 'wind_speed')\n",
    "\n",
    "for item in beaufort:\n",
    "    test.loc[(test['wind_speed']>=item[1]) & (test['wind_speed']<item[2]), 'beaufort_scale'] = item[0]\n",
    "\n",
    "    \n",
    "test = test[feat_cols]\n",
    "test, NAlist = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 834/834 [08:43<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "i=0\n",
    "res = np.zeros((test.shape[0]),dtype=np.float32)\n",
    "step_size = 50000\n",
    "for j in tqdm(range(int(np.ceil(test.shape[0]/step_size)))):\n",
    "    for_prediction = get_keras_data(test.iloc[i:i+step_size], numericals, categoricals)\n",
    "    res[i:min(i+step_size,test.shape[0])] = \\\n",
    "       np.expm1(sum([model.predict(for_prediction, batch_size=1024)[:,0] for model in models])/folds)\n",
    "    i+=step_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING THE SUBMISSION FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>169.929565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>86.014809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15.346853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>247.700104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>836.709900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697595</td>\n",
       "      <td>41697595</td>\n",
       "      <td>9.887867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697596</td>\n",
       "      <td>41697596</td>\n",
       "      <td>7.708518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697597</td>\n",
       "      <td>41697597</td>\n",
       "      <td>4.153594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697598</td>\n",
       "      <td>41697598</td>\n",
       "      <td>221.216507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41697599</td>\n",
       "      <td>41697599</td>\n",
       "      <td>5.038878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id  meter_reading\n",
       "0                0     169.929565\n",
       "1                1      86.014809\n",
       "2                2      15.346853\n",
       "3                3     247.700104\n",
       "4                4     836.709900\n",
       "...            ...            ...\n",
       "41697595  41697595       9.887867\n",
       "41697596  41697596       7.708518\n",
       "41697597  41697597       4.153594\n",
       "41697598  41697598     221.216507\n",
       "41697599  41697599       5.038878\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('/Users/ingle/JUPYTER/123/sample_submission.csv')\n",
    "submission['meter_reading'] = res\n",
    "submission.loc[submission['meter_reading']<0, 'meter_reading'] = 0\n",
    "submission.to_csv('my_submission1.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF THE CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
